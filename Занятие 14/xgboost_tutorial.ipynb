{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>XGBoost</font> eXtreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/dmlc/xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем датасет Boston Housing и обучим XGBoost на нем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(31337)\n",
    "\n",
    "boston = load_boston()\n",
    "y = boston['target']\n",
    "X = boston['data']\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost предлагает 2 способа использования алгоритмов:\n",
    "* sklearn-совместимые классы XGBClassifier, XGBRegressor\n",
    "\n",
    "* \"оригинальная\" python-библиотека"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:07:18] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "RMSE on fold 0: 4.7144853667812905\n",
      "[20:07:18] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "RMSE on fold 1: 3.1474422125618693\n"
     ]
    }
   ],
   "source": [
    "for fold_index, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    xgb_model = xgb.XGBRegressor().fit(X[train_index], y[train_index])\n",
    "    predictions = xgb_model.predict(X[test_index])\n",
    "    actuals = y[test_index]\n",
    "    print(\"RMSE on fold {}: {}\".format(fold_index, np.sqrt(mean_squared_error(actuals, predictions))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:16:00] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "RMSE on fold 0: [0]\teval-rmse:8.714170\n",
      "[20:16:00] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "RMSE on fold 1: [0]\teval-rmse:7.936395\n"
     ]
    }
   ],
   "source": [
    "def get_params():\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"reg:linear\"\n",
    "    params[\"booster\"] = \"gbtree\"\n",
    "    params[\"eval_metric\"] = \"rmse\"\n",
    "    params[\"num_boost_round\"] = 100\n",
    "    params[\"max_depth\"] = 3\n",
    "    params[\"tree_method\"] = \"approx\"\n",
    "    params[\"sketch_eps\"] = 1\n",
    "    \n",
    "    return params\n",
    "    \n",
    "for fold_index, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "\n",
    "    params = get_params()\n",
    "    \n",
    "    xgtrain = xgb.DMatrix(X[train_index], label=y[train_index])\n",
    "    xgtest = xgb.DMatrix(X[test_index], label=y[test_index])\n",
    "\n",
    "    bst = xgb.train(params, xgtrain)\n",
    "\n",
    "    print(\"RMSE on fold {}: {}\".format(fold_index, bst.eval(xgtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный бустинг на решающих деревьях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Хотим построить композицию алгоритмов:\n",
    "<font size=5>\n",
    "\n",
    "$$ \\hat{y_i} = \\phi(x_i) = \\sum_{k=1}^{K} f_k(x_i) $$\n",
    "\n",
    "$$ Obj(f) = \\sum_{i} l(y_i, \\hat{y_i} ) + \\sum_k \\Omega(f_k)$$\n",
    "\n",
    "$$ \\Omega(f_k) = \\gamma T + \\frac{1}{2}\\lambda\\sum_{j=1}^{T}w_j^2 + \\alpha\\sum_{j=1}^{T}w_j$$\n",
    "\n",
    "\n",
    "<font size=3>\n",
    "\n",
    "$ x_i, y_i, \\hat{y_i} $ - i-ый объект, правильный ответ и предсказание модели для для него\n",
    "\n",
    "$ \\Omega $ - регуляризация\n",
    "\n",
    "T - количество листьев в дереве\n",
    "\n",
    "$ w_j $ - веса, проставленные в листьях дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Преимущества:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* потенциально очень высокое качество во многих задачах\n",
    "\n",
    "* находит нелинейные связи\n",
    "\n",
    "* способен обработать датасеты с большим числом объектов и признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Недостатки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* очень много параметров\n",
    "\n",
    "* модели не интерпретируемы\n",
    "\n",
    "* по умолчанию не очень быстрый"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Особенности XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "Написан на C++, есть обертки на Python, R, Java, Scala\n",
    "\n",
    "С помощью XGBoost выиграна половина конкурсов на Kaggle\n",
    "\n",
    "Существует коммерческая версия TreeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "Для уменьшения переобучения целевая функция поддерживает L0, L1, L2 регуляризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Параллелизм (по признакам)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://zhanpengfang.github.io/fig_418/feature_speedup.png\" height=\"600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "Также есть возможность запускаться на Hadoop, Spark, Flink и DataFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кастомные функции потерь / метрики качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В XGBoost встроено множество различных функций потерь:\n",
    "\n",
    "* reg:linear\n",
    "\n",
    "* reg:logistic\n",
    "\n",
    "* binary:logistic\n",
    "\n",
    "* binary:logitraw\n",
    "\n",
    "* multi:softmax\n",
    "\n",
    "* rank:pairwise\n",
    "\n",
    "* ...\n",
    "\n",
    "А также соответствующих eval_metric, которые замеряют качество и позволяют сделать early stop.\n",
    "\n",
    "Но также имеется возможность реализовать свой objective и eval_metric.\n",
    "\n",
    "Все, что для этого нужно - уметь считать градиент и гессиан."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_reg_linear(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    grad = (preds - labels)\n",
    "    hess = np.ones(labels.shape[0])\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:16:14] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[0]\teval-rmse:7.770299\n",
      "[20:16:14] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[0]\teval-rmse:7.848326\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    params = get_params()\n",
    "    \n",
    "    xgtrain = xgb.DMatrix(X[train_index], label=y[train_index])\n",
    "    xgtest = xgb.DMatrix(X[test_index], label=y[test_index])\n",
    "\n",
    "    bst = xgb.train(params, xgtrain, obj=my_reg_linear)\n",
    "    \n",
    "    predictions = bst.predict(xgtest)\n",
    "    actuals = y[test_index]\n",
    "\n",
    "    print(bst.eval(xgtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximated tree splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если данных слишком много, то можно использовать не все значения признаков, а разделить их на бакеты.\n",
    "\n",
    "А именно, от каждого признака берутся не все значения, а только некоторое подмножество. Разбиение производится по элементам этого подмножества. \n",
    "\n",
    "Для разбиения выбираются взвешенные перцентили.\n",
    "\n",
    "В оригинальной статье указывается 2 алгоритма:\n",
    "*   глобальный - один раз выбрать разбиение значений фактора перед началом построения дерева и зафиксировать\n",
    "\n",
    "    экономим на выборе разбиений, но обычно приходится выбирать больше точек разбиения\n",
    "    \n",
    "    \n",
    "*   локальный - выбирать разбиение после каждого сплита\n",
    "  \n",
    "    работает лучше на глубоких деревьях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"tree_method\"] = \"approx\"\n",
    "params[\"sketch_eps\"] = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пропуски в данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost умеет обрабатывать разреженные матрицы\n",
    "\n",
    "Но категориальные признаки нужно приводить к числовому виду\n",
    "\n",
    "Нужно указать, какое число является \"пропуском\"\n",
    "\n",
    "При сплите, алгоритм смотрит в какую сторону лучше отвести объекты с пропуском."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgtrain_missed = xgb.DMatrix(X[test_index], label=y[test_index], missing=-999.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитывает сколько раз каждый признак использовался для использовался в вершине дерева при разбиении\n",
    "\n",
    "Это не качество фактора, а его важность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f8': 8,\n",
       " 'f4': 10,\n",
       " 'f2': 5,\n",
       " 'f0': 3,\n",
       " 'f10': 4,\n",
       " 'f11': 8,\n",
       " 'f6': 10,\n",
       " 'f3': 5,\n",
       " 'f12': 1,\n",
       " 'f7': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d9826e8160>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "xgb.plot_importance(bst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прунинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно GBM перестает разделять вершины дерева, когда gain становится отрицательным - жадный подход.\n",
    "Могло оказаться так, что после неудачного сплита с отрицательным gain'ом получится сделать сильно положительный сплит.\n",
    "\n",
    "XGBoost доводит деревья до max_depth, после чего начинает удалять сплиты, которые несут отрицательный вклад."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дообучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:19:53] 6513x127 matrix with 143286 entries loaded from agaricus.txt.train\n",
      "[20:19:53] 1611x127 matrix with 35442 entries loaded from agaricus.txt.test\n",
      "start running example to start from a initial prediction\n",
      "[0]\teval-error:0.042831\ttrain-error:0.046522\n",
      "this is result of running from initial prediction\n",
      "[0]\teval-error:0.021726\ttrain-error:0.022263\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix('agaricus.txt.train')\n",
    "dtest = xgb.DMatrix('agaricus.txt.test')\n",
    "watchlist  = [(dtest,'eval'), (dtrain,'train')]\n",
    "###\n",
    "# advanced: start from a initial base prediction\n",
    "#\n",
    "print ('start running example to start from a initial prediction')\n",
    "# specify parameters via map, definition are same as c++ version\n",
    "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "# train xgboost for 1 round\n",
    "bst = xgb.train( param, dtrain, 1, watchlist )\n",
    "\n",
    "# Note: we need the margin value instead of transformed prediction in set_base_margin\n",
    "# do predict with output_margin=True, will always give you margin values before logistic transformation\n",
    "ptrain = bst.predict(dtrain, output_margin=True)\n",
    "ptest  = bst.predict(dtest, output_margin=True)\n",
    "\n",
    "dtrain.set_base_margin(ptrain)\n",
    "dtest.set_base_margin(ptest)\n",
    "\n",
    "print ('this is result of running from initial prediction')\n",
    "bst = xgb.train( param, dtrain, 1, watchlist )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Встроенная кросс валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022263</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.022263</td>\n",
       "      <td>0.002821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.001405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.001809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.001405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>0.001097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.000614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.000614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.000614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.000798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.000921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.000921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.000798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.000798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.000798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0           0.022263         0.000940         0.022263        0.002821\n",
       "1           0.007063         0.000469         0.007063        0.001405\n",
       "2           0.015200         0.000603         0.015200        0.001809\n",
       "3           0.007063         0.000469         0.007063        0.001405\n",
       "4           0.001945         0.001369         0.001996        0.001097\n",
       "5           0.001228         0.000205         0.001228        0.000614\n",
       "6           0.001228         0.000205         0.001228        0.000614\n",
       "7           0.001228         0.000205         0.001228        0.000614\n",
       "8           0.000870         0.000529         0.001075        0.000798\n",
       "9           0.000512         0.000512         0.000921        0.000921\n",
       "10          0.000409         0.000434         0.000921        0.000921\n",
       "11          0.000153         0.000266         0.000461        0.000798\n",
       "12          0.000153         0.000266         0.000461        0.000798\n",
       "13          0.000153         0.000266         0.000461        0.000798\n",
       "14          0.000000         0.000000         0.000000        0.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.cv(param, dtrain, nfold = 4, num_boost_round=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 вида бустеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gbtree - обычные решающие деревья\n",
    "\n",
    "* gblinear - линейные модели\n",
    "\n",
    "* dart - решающие деревья, алгоритм может \"выбрасывать\" некоторые из деревьев, уменьшая переобучение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Веса для объектов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем учитывать каждый объект со своим весом, этот вес будет учитываться и при выборе бакетов при приближенном построении деревьев, при сплите, при подсчете Objective.\n",
    "\n",
    "Допустим, мы хотим классифицировать короткие сообщения.  Некоторые из них повторяются. В этом случае выгодно \"слить\" вместе все дубликаты и посчитать их один раз, но с большим весом. При неизменном качестве это уменьшит время обучения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:24:05] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Repeated dataset. Train size: 739, error: [0]\teval-rmse:6.486039\n",
      "[20:24:05] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Weighted dataset. Train size: 300, error: [0]\teval-rmse:6.486039\n"
     ]
    }
   ],
   "source": [
    "repeats = np.random.randint(low=1, high=5, size=X.shape[0])\n",
    "train_examples = 300\n",
    "\n",
    "\n",
    "X_train = X[:train_examples]\n",
    "X_test = X[train_examples:]\n",
    "y_train = y[:train_examples]\n",
    "y_test = y[train_examples:]\n",
    "\n",
    "\n",
    "X_train_repeated = np.repeat(X_train, repeats[:train_examples], axis=0)\n",
    "X_test_repeated = np.repeat(X_test, repeats[train_examples:], axis=0)\n",
    "y_train_repeated = np.repeat(y_train, repeats[:train_examples], axis=0)\n",
    "\n",
    "\n",
    "xgtrain_repeated = xgb.DMatrix(X_train_repeated, label=y_train_repeated)\n",
    "xgtrain_weighted = xgb.DMatrix(X_train, label=y_train, weight=repeats[:train_examples])\n",
    "\n",
    "xgtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "bst = xgb.train(params, xgtrain_repeated)\n",
    "print(\"Repeated dataset. Train size: {}, error: {}\".format(xgtrain_repeated.num_row(), bst.eval(xgtest)))\n",
    "\n",
    "bst = xgb.train(params, xgtrain_weighted)\n",
    "print(\"Weighted dataset. Train size: {}, error: {}\".format(xgtrain_weighted.num_row(), bst.eval(xgtest)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Другие параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> learning_rates </i> - можно настроить убывающую скорость"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры деревьев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "<i> max_depth </i> - максимальная глубина дерева. Слишком большая глубина ведет к переобучению\n",
    "\n",
    "<i> subsample, colsample_bytree, colsample_bylevel </i> - сэмплирование по объектам и признакам\n",
    "\n",
    "\n",
    "<i> min_child_weight </i> - минимальная сумма весов в листе\n",
    "\n",
    "<i> scale_pos_weight </i> - вес целого класса, используется если один класс заметно чаще встречается, чем другой\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительные параметры для DART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "<i> sample_type </i> - стратегия выбора деревьев для выкидывания\n",
    "\n",
    "<i> rate_drop </i> - какую долю выкидываем\n",
    "\n",
    "<i> skip_drop </i> - шанс пропустить дроп на этой итерации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Настраиваем XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "* Выбираем относительно большую learning_rate ($ \\eta \\in [0.05, 0.3]$), подбираем оптимальное число деревьев для выбранного $ \\eta $\n",
    "\n",
    "* Настраиваем параметры деревьев, начиная с самых значимых (max_depth, min_child_weight, gamma, subsample, colsample_bytree)\n",
    "\n",
    "* Настраиваем регуляризации ($ \\lambda, \\alpha $)\n",
    "\n",
    "* Уменьшаем learning_rate, пропорционально увеличиваем число деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
